# キャリアバンクジョブサーチのスクレイピング

## スクレイピングフロー
1. https://careerbank-jobsearch.com/page/{page_number}にアクセス
2. 1ページ50件の求人情報を参照
3. 全てのページで処理を行う
4. 全ての求人情報をcsvとして出力
5. csvを読み取り1件ずつ詳細ページの移動し必要情報を抽出する
6. 全ての求人のデータをマージしcsvとして出力する
7. csvを読み取り500件ずつDBに挿入する

## 実行方法
### ライブラリインストール
必要なライブラリをインストールしてください。
その際はグローバル空間へのダウンロードにならないようvenvなどで仮想環境を作成してください。

参考リンク: https://qiita.com/fiftystorm36/items/b2fd47cf32c7694adc2e
```sh
pip3 install -r requirements.txt
```

### Cookieの取得
まずはブラウザ上でキャリアバンクジョブサーチに手動でログインしてください。
ログインした状態でブラウザの開発ツールのNetworkタブを開いてください。そこの一番上にあるネットワークをクリックし以下の手順でCookieを確認します。
Headersタブ > Request Headersセクション > Cookie
参考: https://qiita.com/shizen-shin/items/b619586a293de8cfe7da

Cookieの右側に「wordpress_test_cookie=...」から始まる文字列があるのでそれをコピーして取っておいてください。

### スクレイピング実行
以下何度か「コマンド実行」「ファイル出力」を行います。理由としては以下の3点です。
- 処理件数が多いのでPCのメモリサイズによっては全ての処理を行うと処理が途中で中断されてしまう可能性があること
- ログインセッションが切れる場合があること
- サイトの処理が高速ではないのでスクレイピングにかなりの時間がかかること


このコマンドは求人一覧ページから全ての求人の基本情報を取得しcsv出力します。
```sh
python3 list.py -p <求人一覧のページ数> -c "<先ほどコピーしたcookie>"
```
※ 求人一覧のページ数は2025/02/16時点で138ですが、実行時のページ数を入力ください。
実行すると `job_list.csv` というファイルが出力されます。このファイルは次のコマンド実行時に利用されます。

このコマンドはcsvファイルを読み取り求人詳細ページから情報を取得しcsv出力します。
```sh
python3 detail.py -f job_list.csv -m <処理するレコード数>　-c "<先ほどコピーしたcookie>"
```
※ `-m <処理するレコード数>`を省略すると全件のレコードが処理されます。場合によっては大量の時間がかかります。
実行すると `jobs_detail.csv` というファイルが出力されます。このファイルは次のコマンド実行時に利用されます。

このコマンドはcsvファイルを読み取りデータベースにデータを挿入します。
```sh
python3 insert.py -f jobs_detail.csv --dbname "<データベース名>" --user "<データベースユーザー名>" --password "<データベースパスワード>" --host "<データベースのホスト>" --port 5432
```
